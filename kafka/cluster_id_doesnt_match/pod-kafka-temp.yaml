kind: Pod
apiVersion: v1
metadata:
  name: kafka-temp
  namespace: kafka-cluster-2
spec:
  restartPolicy: Never
  serviceAccountName: default
  imagePullSecrets:
    - name: default-dockercfg-t2vgh
  priority: 0
  schedulerName: default-scheduler
  enableServiceLinks: true
  terminationGracePeriodSeconds: 30
  preemptionPolicy: PreemptLowerPriority
  nodeName: <IFILL WITH WORKER NODE NAME>
  securityContext:
    seLinuxOptions:
      level: 's0:c26,c10'
  containers:
    - resources:
        limits:
          cpu: 500m
          memory: 1536Mi
        requests:
          cpu: 50m
          memory: 256Mi
      terminationMessagePath: /dev/termination-log
      name: kafka-temp
      securityContext:
        capabilities:
          drop:
            - MKNOD
      imagePullPolicy: IfNotPresent
      volumeMounts:
        - name: kube-api-access-vk8dz
          readOnly: true
          mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        - volumeMounts:
          mountPath: "/var/lib/kafka/data-0"
          name: kafka-0             
      terminationMessagePolicy: File
      image: 'quay.io/strimzi/kafka:latest-kafka-3.6.0'
      args:
        - /bin/sh
        - '-c'
        - sleep 86400
  serviceAccount: default
  volumes:
    - name: kube-api-access-vk8dz
      projected:
        sources:
          - serviceAccountToken:
              expirationSeconds: 3607
              path: token
          - configMap:
              name: kube-root-ca.crt
              items:
                - key: ca.crt
                  path: ca.crt
          - downwardAPI:
              items:
                - path: namespace
                  fieldRef:
                    apiVersion: v1
                    fieldPath: metadata.namespace
          - configMap:
              name: openshift-service-ca.crt
              items:
                - key: service-ca.crt
                  path: service-ca.crt
        defaultMode: 420
    - name: kafka-0
      persistentVolumeClaim:  
        claimName: data-0-amq-streams-lab-kafka-0          
  dnsPolicy: ClusterFirst
  tolerations:
    - key: node.kubernetes.io/not-ready
      operator: Exists
      effect: NoExecute
      tolerationSeconds: 300
    - key: node.kubernetes.io/unreachable
      operator: Exists
      effect: NoExecute
      tolerationSeconds: 300
    - key: node.kubernetes.io/memory-pressure
      operator: Exists
      effect: NoSchedule
```



Este erro pode tanto estar localizaodo em apenas um broker, quanto em v√°rios brokers do cluster